<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Gentle intro to logistic regression</title>
  
  <meta property="description" itemprop="description" content="A gentle, step-by-step intro to logistic regression, inverse logit and logit functions, and maximum likelihood estimation in the context of logistic regression"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-04-13"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-04-13"/>
  <meta name="article:author" content="Hause Lin"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Gentle intro to logistic regression"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="A gentle, step-by-step intro to logistic regression, inverse logit and logit functions, and maximum likelihood estimation in the context of logistic regression"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Gentle intro to logistic regression"/>
  <meta property="twitter:description" content="A gentle, step-by-step intro to logistic regression, inverse logit and logit functions, and maximum likelihood estimation in the context of logistic regression"/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","categories","output","draft","repository_url","editor_options"]}},"value":[{"type":"character","attributes":{},"value":["Gentle intro to logistic regression"]},{"type":"character","attributes":{},"value":["A gentle, step-by-step intro to logistic regression, inverse logit and logit functions, and maximum likelihood estimation in the context of logistic regression\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Hause Lin"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":[]}},"value":[]}]}]},{"type":"character","attributes":{},"value":["04-13-2019"]},{"type":"character","attributes":{},"value":["resources","logistic regression","logit","general linear model","maximum likelihood"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["toc","self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[true]},{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["https://github.com/hauselin/rtutorialsite/blob/master/_posts/2019-04-13-logistic-regression/logistic-regression.Rmd"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["chunk_output_type"]}},"value":[{"type":"character","attributes":{},"value":["console"]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["logistic-regression_files/bowser-1.9.3/bowser.min.js","logistic-regression_files/distill-2.2.21/template.v2.js","logistic-regression_files/figure-html5/unnamed-chunk-12-1.png","logistic-regression_files/figure-html5/unnamed-chunk-13-1.png","logistic-regression_files/figure-html5/unnamed-chunk-13-2.png","logistic-regression_files/figure-html5/unnamed-chunk-15-1.png","logistic-regression_files/figure-html5/unnamed-chunk-16-1.png","logistic-regression_files/figure-html5/unnamed-chunk-17-1.png","logistic-regression_files/figure-html5/unnamed-chunk-18-1.png","logistic-regression_files/figure-html5/unnamed-chunk-19-1.png","logistic-regression_files/figure-html5/unnamed-chunk-2-1.png","logistic-regression_files/figure-html5/unnamed-chunk-20-1.png","logistic-regression_files/figure-html5/unnamed-chunk-21-1.png","logistic-regression_files/figure-html5/unnamed-chunk-34-1.png","logistic-regression_files/figure-html5/unnamed-chunk-5-1.png","logistic-regression_files/figure-markdown_github/unnamed-chunk-5-1.png","logistic-regression_files/jquery-1.11.3/jquery.min.js","logistic-regression_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="logistic-regression_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="logistic-regression_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="logistic-regression_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="logistic-regression_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Gentle intro to logistic regression","description":"A gentle, step-by-step intro to logistic regression, inverse logit and logit functions, and maximum likelihood estimation in the context of logistic regression","authors":[{"author":"Hause Lin","authorURL":{},"affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2019-04-13T00:00:00.000+02:00","citationText":"Lin, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Gentle intro to logistic regression</h1>
<p>A gentle, step-by-step intro to logistic regression, inverse logit and logit functions, and maximum likelihood estimation in the context of logistic regression</p>
</div>

<div class="d-byline">
  Hause Lin true 
  
<br/>04-13-2019
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#logistic-sigmoid-or-inverse-logit-function">Logistic (sigmoid or inverse logit) function</a></li>
<li><a href="#linear-regression-basics">Linear regression basics</a></li>
<li><a href="#from-linear-to-logistic-regression-via-the-logistic-function">From linear to logistic regression via the logistic function</a></li>
<li><a href="#inverse-logit-and-logit-functions">Inverse logit and logit functions</a><ul>
<li><a href="#summary-inverse-logit-and-logit-functions">Summary: inverse logit and logit functions</a></li>
</ul></li>
<li><a href="#understanding-logistic-regression">Understanding logistic regression</a><ul>
<li><a href="#log-odds">Log odds</a></li>
<li><a href="#interpretating-logistic-regression-coefficients">Interpretating logistic regression coefficients</a></li>
</ul></li>
<li><a href="#fitting-logistic-regression-in-r">Fitting logistic regression in R</a></li>
<li><a href="#how-are-logistic-regression-coefficients-estimated">How are logistic regression coefficients estimated?</a></li>
<li><a href="#another-logistic-regression-example">Another logistic regression example</a></li>
<li><a href="#step-by-step-algebra-convert-between-inverse-logit-and-logit-equations">Step-by-step algebra: convert between inverse logit and logit equations</a><ul>
<li><a href="#inverse-logit-to-logit">Inverse logit to logit</a></li>
<li><a href="#logit-to-inverse-logit">Logit to inverse logit</a></li>
</ul></li>
<li><a href="#further-resources">Further resources</a></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<p>Get source code for this RMarkdown script <a href="https://github.com/hauselin/rtutorialsite/blob/master/_posts/2019-04-13-logistic-regression/logistic-regression.Rmd">here</a>.</p>
<p>Logistic regression (also known as classification in machine learning) is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (e.g., yes, success) or 0 (e.g., no, failure).</p>
<p>I have read many explanations and tutorials but haven’t found one that clearly explains how logistic regression, inverse logit and logit functions, and maximum likelihood estimation are related. I hope this tutorial will fill those gaps. If anything is unclear or wrong, let me know.</p>
<p>The principles underlying logistic regression are also relevant to <a href="https://khakieconomics.github.io/2019/03/17/A-simple-model-of-choice.html">decision making and choice models</a>, machine learning, <a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">activation functions in neural networks</a>, and many other things. So it’s good to understand the basic underlying principles really well.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(tidyverse); library(data.table)</code></pre>
</div>
<h2 id="logistic-sigmoid-or-inverse-logit-function">Logistic (sigmoid or inverse logit) function</h2>
<p>The logistic function (also known as <strong>sigmoid function</strong> or <strong>inverse logit function</strong>) is at the heart of logistic regression. Logistic function:</p>
<aside>
We’ll get to the (non-inverse) logit function later on.
</aside>
<p><span class="math display">\[f(x)=\frac{1}{1+e^{-x}}\]</span> Another formula for logistic function:</p>
<p><span class="math display">\[g(x)=\frac{e^{x}}{e^{x}+1}\]</span> You can easily derive <span class="math inline">\(g(x)\)</span> from <span class="math inline">\(f(x)\)</span>:</p>
<p><span class="math display">\[\frac{1}{1+e^{-x}}=
(\frac{1}{1+e^{-x}}) * (\frac{e^{x}}{e^{x}})
=\frac{e^{x}}{e^{x}+1}
\]</span></p>
<p>Define logistic (sigmoid) R functions:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
logistic &lt;- function(x) { # f(x)
  1 / (1 + exp(-x))
}

logistic2 &lt;- function(x) { # g(x)
  exp(x) / (1 + exp(x))
}</code></pre>
</div>
<p>Let’s create a table to see how the logistic functions map input values -3, -2, -1, 0, 1, 2, 3 to output values.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
tibble(x = -3:3, f_x = logistic(x), g_x = logistic2(x))</code></pre>
<pre><code>
# A tibble: 7 x 3
      x    f_x    g_x
  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;
1    -3 0.0474 0.0474
2    -2 0.119  0.119 
3    -1 0.269  0.269 
4     0 0.5    0.5   
5     1 0.731  0.731 
6     2 0.881  0.881 
7     3 0.953  0.953 </code></pre>
</div>
<p>They are the same equations, so it’s not surprising that the output values of the two functions are exactly the same.</p>
<p>Here are more interesting input values:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
logistic(-Inf) # -Inf -&gt; 0</code></pre>
<pre><code>
[1] 0</code></pre>
<pre class="r"><code>
logistic(Inf) # Inf -&gt; 1</code></pre>
<pre><code>
[1] 1</code></pre>
<pre class="r"><code>
logistic(0) # 0 -&gt; 0.5, because exp(0) is 1, thus 1/(1+1)</code></pre>
<pre><code>
[1] 0.5</code></pre>
</div>
<p>The logistic function, also called sigmoid function gives an ‘S’ shaped curve that can take any real-valued number (-<span class="math inline">\(\infty\)</span> to <span class="math inline">\(+\infty\)</span>) and maps it to value between 0 and 1.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
tibble(x = seq(-20, 20, 0.1), y = logistic(x)) %&gt;% 
  ggplot(aes(x, y)) + geom_point() +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0.5) +
  labs(x = &quot;x (input values)&quot;, 
       y = &quot;y or f(x) (output or transformed values)&quot;, 
       title = &quot;Logistic (sigmoid) function&quot;)</code></pre>
<p><img src="logistic-regression_files/figure-html5/unnamed-chunk-5-1.png" width="624" /></p>
</div>
<h2 id="linear-regression-basics">Linear regression basics</h2>
<p>Let’s create some simple data. Let x be the predictor and y become the outcome.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dt1 &lt;- data.table(x = c(-5, -3, -1, 0, 1, 3, 5),# predictor x
                  y = c( 0,  0,  1, 0, 1, 1, 1)) # outcome y
dt1</code></pre>
<pre><code>
    x y
1: -5 0
2: -3 0
3: -1 1
4:  0 0
5:  1 1
6:  3 1
7:  5 1</code></pre>
</div>
<p>Let’s fit a simple linear regression to model y in terms of x. Note that using linear regression is wrong because y is categorical/binary (0s and 1s). But let’s fit the model regardless…</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
lm1 &lt;- lm(y ~ x, dt1)
lm1</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = dt1)

Coefficients:
(Intercept)            x  
     0.5714       0.1143  </code></pre>
</div>
<p>Based on these results, we’re saying we can model each y value (<span class="math inline">\(y_i\)</span>) as a function of the corresponding x value (<span class="math inline">\(x_i\)</span>), using this linear regression function:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_i + \varepsilon_i\]</span></p>
<p>Matrix notation:</p>
<p><span class="math display">\[
\left(\begin{array}{cc} 
y_1\\
y_2\\ 
\vdots\\
y_n\\
\end{array}\right)
=
\left(\begin{array}{cc} 
1 &amp; x_1\\
1 &amp; x_2\\
\vdots &amp; \vdots\\
1 &amp; x_n\\
\end{array}\right)
\left(\begin{array}{cc} 
\beta_0\\ 
\beta_1
\end{array}\right)
+
\left(\begin{array}{cc} 
\ \varepsilon_1\\
\ \varepsilon_2\\ 
\vdots\\
\ \varepsilon_n
\end{array}\right)
\]</span></p>
<p>Let’s insert the fitted <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values into the equation and then check whether we can map our x values to our y values correctly. <span class="math inline">\(\beta_0\)</span> = 0.57 and <span class="math inline">\(\beta_1\)</span> = 0.11.</p>
<p>First, let’s extract the beta coefficients from the model, which we can then use to calculate predicted values, <span class="math inline">\(\hat{Y}\)</span>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# extract b0, b1 coefficients, residuals and save in dt1 (data.table syntax!)
dt1[, b0 := round(coef(lm1)[1], 4)]
dt1[, b1 := round(coef(lm1)[2], 4)]
dt1</code></pre>
<pre><code>
    x y     b0     b1
1: -5 0 0.5714 0.1143
2: -3 0 0.5714 0.1143
3: -1 1 0.5714 0.1143
4:  0 0 0.5714 0.1143
5:  1 1 0.5714 0.1143
6:  3 1 0.5714 0.1143
7:  5 1 0.5714 0.1143</code></pre>
</div>
<p>Matrix notation is shown below. This notation is nice because it’s compact and no matter how many <span class="math inline">\(x\)</span> variables (predictors) you have in your model, the form is still the same.</p>
<p><span class="math display">\[\hat{Y}=X \hat{\beta}\]</span></p>
<p>For example, row 3: <code>x</code> = -1:</p>
<p><span class="math display">\[\hat{y_3} = \beta_0 + \beta_1x_3 \]</span> <span class="math display">\[\hat{y_3} = 0.57 + 0.11 * -1\]</span></p>
<p>Let’s do it for all rows.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# map x values to y (predicted y or y_hat)
dt1[, y_hat := round(b0 + b1 * x, 4)] # predicted_y = b0 + b1*x
dt1</code></pre>
<pre><code>
    x y     b0     b1   y_hat
1: -5 0 0.5714 0.1143 -0.0001
2: -3 0 0.5714 0.1143  0.2285
3: -1 1 0.5714 0.1143  0.4571
4:  0 0 0.5714 0.1143  0.5714
5:  1 1 0.5714 0.1143  0.6857
6:  3 1 0.5714 0.1143  0.9143
7:  5 1 0.5714 0.1143  1.1429</code></pre>
</div>
<p>To get the original <span class="math inline">\(Y\)</span> values (0s and 1s), we add our predicted y values <span class="math inline">\(\hat{Y}\)</span> to the residuals <span class="math inline">\(\varepsilon\)</span>.</p>
<p><span class="math display">\[Y=X\beta + \varepsilon\]</span></p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# save residuals in dt1
dt1[, resids := round(residuals(lm1), 4)]
dt1[, y2 := round(y_hat + resids, 2)] # y2 should be same as y
dt1 </code></pre>
<pre><code>
    x y     b0     b1   y_hat  resids y2
1: -5 0 0.5714 0.1143 -0.0001  0.0000  0
2: -3 0 0.5714 0.1143  0.2285 -0.2286  0
3: -1 1 0.5714 0.1143  0.4571  0.5429  1
4:  0 0 0.5714 0.1143  0.5714 -0.5714  0
5:  1 1 0.5714 0.1143  0.6857  0.3143  1
6:  3 1 0.5714 0.1143  0.9143  0.0857  1
7:  5 1 0.5714 0.1143  1.1429 -0.1429  1</code></pre>
</div>
<p>The <code>y</code> and <code>y2</code> values match exactly! We recreated our <code>y</code> values by using the beta coefficients, x values, and residuals.</p>
<p>For example, row 3: <code>x</code> = -1, <code>y</code> = 1, <span class="math inline">\(\varepsilon\)</span> = 0.54:</p>
<p><span class="math display">\[y_3 = \beta_0 + \beta_1x_3 + \varepsilon_3\]</span> <span class="math display">\[y_3 = 0.57 + 0.11 * -1 + 0.54 =  1\]</span></p>
<h2 id="from-linear-to-logistic-regression-via-the-logistic-function">From linear to logistic regression via the logistic function</h2>
<p>Below is the <strong>logistic function</strong> (aka <strong>sigmoid function</strong>, <strong>inverse logit link function</strong>) again. Note I’ve replaced the <span class="math inline">\(x\)</span> variable with the <span class="math inline">\(y\)</span> variable and changed the function name from <span class="math inline">\(f\)</span> to <span class="math inline">\(logistic\)</span>. Other than that, it’s the same equation as above.</p>
<p><span class="math display">\[logistic(y)=\frac{1}{1+e^{-y}}\]</span></p>
<p>Remember the matrix notation for the general linear model? It’s <span class="math inline">\(Y=X\beta\)</span>. In logistic regression, here’s what we’re trying to model:</p>
<p><span class="math display">\[logistic(X\beta)=\frac{1}{1+e^{-X\beta}}\]</span> Or the other formula:</p>
<p><span class="math display">\[logistic(X\beta)=\frac{e^{X\beta}}{e^{X\beta}+1}\]</span></p>
<p>The input to the logistic function is now the predicted value <span class="math inline">\(Y=X\beta\)</span>.</p>
<p>Remember the logistic function returns continuous values ranging from 0 to 1, so <span class="math inline">\(Y=X\beta\)</span> (can take on any value from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>) will be map on to values between 0 and 1. Let’s illustrate this point again using the <code>y_hat</code> values from our linear regression. Note that what we’re doing here is wrong, because we’ve fitted a linear regression to the data when we should have fitted a linear regression. But I think it’s helpful for understanding conceptually what the logistic function is doing and how logistic regression works.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dt1[, logistic_yhat := logistic(y_hat)] # apply logistic function to y_hat
dt1 # compare the ranges of y_hat and logistic_yhat</code></pre>
<pre><code>
    x y     b0     b1   y_hat  resids y2 logistic_yhat
1: -5 0 0.5714 0.1143 -0.0001  0.0000  0     0.4999750
2: -3 0 0.5714 0.1143  0.2285 -0.2286  0     0.5568777
3: -1 1 0.5714 0.1143  0.4571  0.5429  1     0.6123260
4:  0 0 0.5714 0.1143  0.5714 -0.5714  0     0.6390862
5:  1 1 0.5714 0.1143  0.6857  0.3143  1     0.6650097
6:  3 1 0.5714 0.1143  0.9143  0.0857  1     0.7138793
7:  5 1 0.5714 0.1143  1.1429 -0.1429  1     0.7582117</code></pre>
</div>
<p>Each value in the <code>logistic_yhat</code> column tells you the probability of <span class="math inline">\(y\)</span> equals 1, <span class="math inline">\(P(y_i = 1)\)</span>.</p>
<h2 id="inverse-logit-and-logit-functions">Inverse logit and logit functions</h2>
<p>Here’s the <strong>inverse logit</strong> (aka logistic function or sigmoid function) again. Repetition is good… I’ve changed the left-hand side to <span class="math inline">\(p\)</span>, because the logistic (i.e., inverse logit) function takes inputs from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> and maps those inputs into values ranging from 0 to 1, which are probability values, so <span class="math inline">\(p\)</span> (probability) on the left-hand side makes sense too.</p>
<p><span class="math display">\[p=\frac{1}{1+e^{-y}}\]</span></p>
<p>To remind you how the inverse logit function above maps input values to output values again:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
tibble(x = seq(-20, 20, 0.1), y = logistic(x)) %&gt;% 
  ggplot(aes(x, y)) + geom_point() +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0.5) +
  labs(x = &quot;x (input values)&quot;, y = &quot;p (output or transformed values)&quot;, 
       title = &quot;Logistic (sigmoid or inverse logit) function&quot;)</code></pre>
<p><img src="logistic-regression_files/figure-html5/unnamed-chunk-12-1.png" width="624" /></p>
</div>
<p>What is the <strong>logit function</strong>? It’s the “opposite” or the inverse of the inverse logit function above (inverse-inverse means you undo the inverse!)</p>
<p><span class="math display">\[y=log(\frac{p}{1-p})\]</span></p>
<aside>
This is the natural logarithm. You’ll see different ways of expressing natural logarithm: <span class="math inline">\(log\)</span>, <span class="math inline">\(ln\)</span>, <span class="math inline">\(log_e\)</span>.
</aside>
<p>Define logit function in R:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
logit &lt;- function(p) {
  log(p / (1 - p))
}</code></pre>
</div>
<p>Let’s try a few values to see how the logit function works:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
logit(-3) # warning because you can&#39;t take the log of negative values; try log(-1)</code></pre>
<pre><code>
[1] NaN</code></pre>
<pre class="r"><code>
logit(c(0, 0.5, 1))</code></pre>
<pre><code>
[1] -Inf    0  Inf</code></pre>
<pre class="r"><code>
logit(c(0.001, 0.999))</code></pre>
<pre><code>
[1] -6.906755  6.906755</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
tibble(x = seq(-0.5, 1.5, 0.01), y = logit(x)) %&gt;% 
  ggplot(aes(x, y)) + geom_point() +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0.0) + 
  geom_vline(xintercept = 0.5, linetype = &quot;dashed&quot;) +
  labs(x = &quot;p (input values)&quot;, y = &quot;y or f(x) (output or transformed values)&quot;, 
       title = &quot;Logit function&quot;)</code></pre>
<p><img src="logistic-regression_files/figure-html5/unnamed-chunk-15-1.png" width="624" /></p>
</div>
<p>The logit function maps inputs ranging from 0 to 1 and maps those inputs to values ranging from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. If the input value is smaller than 0 or larger than 1, you get <code>NaN</code> as the output!</p>
<h3 id="summary-inverse-logit-and-logit-functions">Summary: inverse logit and logit functions</h3>
<p><strong>Inverse logit</strong> function (sigmoid or logistic function)</p>
<ul>
<li>input range:<span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span></li>
<li>output range: 0 to 1</li>
</ul>
<p><strong>Logit</strong> function</p>
<ul>
<li>input range: 0 to 1</li>
<li>output range:<span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span></li>
</ul>
<p>Now you should understand why they are “opposites” of each other!</p>
<p>If you’re up for the challenge, work through the algebra, rearrange the terms in the inverse logit function to get the logit function, and vice versa! Solutions are provided below!</p>
<h2 id="understanding-logistic-regression">Understanding logistic regression</h2>
<p>Now we know what the inverse logit and logit functions are, we can finally understand logistic regression. In the general linear model framework, we model y as a function of x using this equation: <span class="math inline">\(\hat{Y}=X \hat{\beta}\)</span></p>
<p>In logistic regression, we apply the logistic function to get the probability of each <span class="math inline">\(y\)</span> value equals 1, <span class="math inline">\(P(y_i = 1)\)</span>.</p>
<p><span class="math display">\[p=\frac{1}{1+e^{-X\beta}}\]</span></p>
<p>And since the equation above can be expressed as <span class="math inline">\(X\beta=log(\frac{p}{1-p})\)</span>, we get the following:</p>
<p><span class="math display">\[log(\frac{p}{1-p})=X\beta\]</span></p>
<aside>
See the section below on converting between inverse logit and logit if you want to see how to convert between these two equations (logit to inverse logit and vice versa).
</aside>
<p>The left-hand side <span class="math inline">\(log(\frac{p}{1-p})\)</span> is the <span class="math inline">\(Y\)</span> we’re modelling when we fit logistic regression. If you replace the left-hand side with <span class="math inline">\(Y\)</span>, you get the basic linear regression equation, <span class="math inline">\(Y=X\beta\)</span>.</p>
<h3 id="log-odds">Log odds</h3>
<p><span class="math inline">\(log(\frac{p}{1-p})\)</span> is called the log odds, because odds is the probability of success <span class="math inline">\(p\)</span> divided by the probability of failure (or non-success), which is <span class="math inline">\(1-p\)</span>, so <span class="math inline">\(odds = \frac{1}{1-p}\)</span>.</p>
<h3 id="interpretating-logistic-regression-coefficients">Interpretating logistic regression coefficients</h3>
<p>Since we’re modelling <span class="math inline">\(log(\frac{p}{1-p})=X\beta\)</span> when we fit logistic regression, the beta coefficients have to be interpreted as such: If <span class="math inline">\(b1 = 0.3\)</span>, when <span class="math inline">\(x_1\)</span> increases by 1 unit, the outcome <span class="math inline">\(log(\frac{p}{1-p})\)</span> increases by 0.3.</p>
<p>But since we aren’t used to interpreting things on the natural log scale (how much is <span class="math inline">\(log(\frac{p}{1-p}) = 0.3\)</span>?), we convert the coefficient back to the “normal scale” by taking the exponential: <span class="math inline">\(e^{log(\frac{p}{1-p})} = e^{0.3}\)</span>, giving us <span class="math inline">\(\frac{p}{1-p} = odds = e^{0.3}\)</span>, which equals 1.3498588. The output is easy to interpret. Let’s try a few <span class="math inline">\(b\)</span> coefficient values:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
exp(c(-2, -0.5, 0, 0.5, 2))</code></pre>
<pre><code>
[1] 0.1353353 0.6065307 1.0000000 1.6487213 7.3890561</code></pre>
</div>
<p>Note that when <span class="math inline">\(b = 0\)</span>, <span class="math inline">\(e^{0} = 1\)</span>, which means the ratio of success to failure <span class="math inline">\(\frac{1}{1-p} = 1\)</span>, so success and failure are equally likely to occur (i.e., null effect). Thus, values smaller than 1 means negative effect, and values larger than 1 means positive effect.</p>
<h2 id="fitting-logistic-regression-in-r">Fitting logistic regression in R</h2>
<p>Here’s out dataset again:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dt1</code></pre>
<pre><code>
    x y     b0     b1   y_hat  resids y2 logistic_yhat
1: -5 0 0.5714 0.1143 -0.0001  0.0000  0     0.4999750
2: -3 0 0.5714 0.1143  0.2285 -0.2286  0     0.5568777
3: -1 1 0.5714 0.1143  0.4571  0.5429  1     0.6123260
4:  0 0 0.5714 0.1143  0.5714 -0.5714  0     0.6390862
5:  1 1 0.5714 0.1143  0.6857  0.3143  1     0.6650097
6:  3 1 0.5714 0.1143  0.9143  0.0857  1     0.7138793
7:  5 1 0.5714 0.1143  1.1429 -0.1429  1     0.7582117</code></pre>
</div>
<p>Fit logistic regression with <code>glm</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
glm1 &lt;- glm(y ~ x, data = dt1, family = binomial(link = &quot;logit&quot;))
# glm1 &lt;- glm(y ~ x, data = dt1, family = binomial()) # also works
summary(glm1) # results</code></pre>
<pre><code>
Call:
glm(formula = y ~ x, family = binomial(link = &quot;logit&quot;), data = dt1)

Deviance Residuals: 
       1         2         3         4         5         6         7  
-0.16193  -0.43185   1.31741  -1.47543   0.58491   0.22307   0.08241  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   0.6779     1.2330   0.550    0.582
x             1.0011     0.7964   1.257    0.209

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9.5607  on 6  degrees of freedom
Residual deviance: 4.5239  on 5  degrees of freedom
AIC: 8.5239

Number of Fisher Scoring iterations: 6</code></pre>
</div>
<p>Model:</p>
<p><span class="math display">\[log(\frac{p}{1-p})= X\beta + \varepsilon = \beta_0 + \beta_1x + \varepsilon\]</span></p>
<p><span class="math display">\[log(\frac{p_i}{1-p_i})= 0.6779 + 1.0011*x_i + \varepsilon_i\]</span></p>
<p>Exponentiate the coefficients to get the values in the “normal” odds scale:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# get coefficients
betas &lt;- coef(glm1)
betas # log(odds) scale</code></pre>
<pre><code>
(Intercept)           x 
  0.6778522   1.0011236 </code></pre>
<pre class="r"><code>
# exponentiate
betas &lt;- round(exp(betas), 3)
betas # odds scale</code></pre>
<pre><code>
(Intercept)           x 
      1.970       2.721 </code></pre>
</div>
<p>Interpretation: 1 unit change in <span class="math inline">\(x\)</span> means the odds of <span class="math inline">\(y = 1\)</span> increases by 2.721.</p>
<p>Here’s a figure to illustrate the results:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ggplot(dt1, aes(x, y)) +
  geom_point() +
  geom_smooth(method = &#39;glm&#39;, method.args = list(family = &quot;binomial&quot;), se = FALSE)</code></pre>
<p><img src="logistic-regression_files/figure-html5/unnamed-chunk-20-1.png" width="624" /></p>
</div>
<h2 id="how-are-logistic-regression-coefficients-estimated">How are logistic regression coefficients estimated?</h2>
<p>Maximum likelihood estimation. I will only briefly describe how it works and won’t go into too much details.</p>
<p>We know what the true coefficient values are from our fitted model:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
(true_values &lt;- coef(glm1))</code></pre>
<pre><code>
(Intercept)           x 
  0.6778522   1.0011236 </code></pre>
</div>
<p>If we have no idea what the coefficients would be, we will guess!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
beta_guess &lt;- c(b0 = 0.5, b1 = 1.0) # guess b0 = 0.5, b1 = 1.0</code></pre>
</div>
<p>With our guesses, we can calculate the expected or predicted <span class="math inline">\(\hat{Y}\)</span> given our coefficients: <span class="math inline">\(\hat{Y}=X\hat{\beta}= \hat{\beta_0} + \hat{\beta_1}x\)</span>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
predicted_y &lt;- beta_guess[&quot;b0&quot;] + beta_guess[&quot;b1&quot;] * dt1$x 
predicted_y </code></pre>
<pre><code>
[1] -4.5 -2.5 -0.5  0.5  1.5  3.5  5.5</code></pre>
</div>
<p>We can get the corresponding predicted probabilities for each <span class="math inline">\(\hat{y_i}\)</span> by applying the inverse logit (or logistic) function:</p>
<p><span class="math display">\[\hat{p}=\frac{1}{1+e^{-\hat{y}}}\]</span></p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
predicted_probs &lt;- logistic(predicted_y)
predicted_probs</code></pre>
<pre><code>
[1] 0.01098694 0.07585818 0.37754067 0.62245933 0.81757448 0.97068777
[7] 0.99592986</code></pre>
</div>
<p>The <code>predicted_probs</code> values are the probabilities of each y value equals 1, given each x value and our beta coefficients (our guesses!).</p>
<p>Since our outcome <span class="math inline">\(y\)</span> contains 0s and 1s, we can assume that the underlying model generating the outcomce variable is the binomial distribution (“coin-toss” distribution), hence the <code>family = binomial(link = &quot;logit&quot;)</code> argument when we use thye <code>glm()</code> function to fit logistic regression.</p>
<p>We can calculate the likelihood of our coefficients <span class="math inline">\(b_0 = 0.5\)</span> and <span class="math inline">\(b_1 = 1.0\)</span> given the observed data <code>dt1$y</code>, which is expressed as the following: <span class="math inline">\(\mathcal{L}(\beta;\boldsymbol{y})\)</span></p>
<p>We can use the <code>dbinom()</code> function to calculate the likelihood of each value in <code>dt1$y</code> (0 or 1) occuring given a probability. Simple examples:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# likelihood of getting 0 when probability of success if 80%
dbinom(x = 0, size = 1, prob = 0.8) </code></pre>
<pre><code>
[1] 0.2</code></pre>
<pre class="r"><code>
# likelihood of getting 0 when probability of success if 40%
dbinom(x = 1, size = 1, prob = 0.4) </code></pre>
<pre><code>
[1] 0.4</code></pre>
</div>
<p>Apply to our data:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
loglikelihood &lt;- dbinom(x = dt1$y, size = 1, prob = predicted_probs, log = TRUE)
# log = TRUE to take the log of the values</code></pre>
</div>
<p>The summed log-likelihood for all our data:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
sum(loglikelihood)</code></pre>
<pre><code>
[1] -2.273334</code></pre>
</div>
<p>We’ve only guessed one set of parameter/coefficient values:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
beta_guess</code></pre>
<pre><code>
 b0  b1 
0.5 1.0 </code></pre>
</div>
<p>Maximum likelihood estimation guesses or estimates many different sets of coefficients and finds the set of coefficient that returns the highest summed log-likelihood.</p>
<p>Let’s create a simple function that calculates the summed log-likelihood for our dataset:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mle &lt;- function(b0 = 0.5, b1 = 1.0) {
  predicted_y &lt;- b0 + b1 * dt1$x 
  predicted_probs &lt;- logistic(predicted_y)
  sum(dbinom(x = dt1$y, size = 1, prob = predicted_probs, log = TRUE))
}</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mle() # default values b0 = 0.5, b1 = 1.0</code></pre>
<pre><code>
[1] -2.273334</code></pre>
<pre class="r"><code>
mle(b0 = 0.6, b1 = 1.01) # another random guess</code></pre>
<pre><code>
[1] -2.264398</code></pre>
</div>
<p>Let’s guess a range of coefficient values and plot!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# create coefficient combinations with expand.grid()
guesses &lt;- expand.grid(b0_guess = seq(0.1, 0.9, 0.05), b1_guess = seq(-2, 2, 0.05)) 
setDT(guesses) # convert to data.table
guesses</code></pre>
<pre><code>
      b0_guess b1_guess
   1:     0.10       -2
   2:     0.15       -2
   3:     0.20       -2
   4:     0.25       -2
   5:     0.30       -2
  ---                  
1373:     0.70        2
1374:     0.75        2
1375:     0.80        2
1376:     0.85        2
1377:     0.90        2</code></pre>
</div>
<p>Since we know what the true values are, let’s include them in our guesses too! Let’s just replace the first row with our true values…</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
guesses[1, b0_guess := true_values[1]]
guesses[1, b1_guess := true_values[2]]
guesses[, guessid := 1:.N] # add an id to each row
guesses</code></pre>
<pre><code>
       b0_guess  b1_guess guessid
   1: 0.6778522  1.001124       1
   2: 0.1500000 -2.000000       2
   3: 0.2000000 -2.000000       3
   4: 0.2500000 -2.000000       4
   5: 0.3000000 -2.000000       5
  ---                            
1373: 0.7000000  2.000000    1373
1374: 0.7500000  2.000000    1374
1375: 0.8000000  2.000000    1375
1376: 0.8500000  2.000000    1376
1377: 0.9000000  2.000000    1377</code></pre>
</div>
<p>Apply our likelihood function to each row and plot the summed log-likelihood values!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# apply our mle() function to each row
guesses[, loglikelihood := mle(b0 = b0_guess, b1 = b1_guess), by = guessid] </code></pre>
</div>
<p>Plot and visualize:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ggplot(guesses, aes(guessid, loglikelihood)) +
  geom_point()</code></pre>
<p><img src="logistic-regression_files/figure-html5/unnamed-chunk-34-1.png" width="624" /></p>
</div>
<p>The first row (or first data point) is our true coefficients obtained from <code>glm()</code> and has the highest summed log-likelihood! All other summed log-likelihood values calculated using other coefficient combinations are less likely to have generated our observed data.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
which.max(guesses$loglikelihood) # row one has the max value</code></pre>
<pre><code>
[1] 1</code></pre>
</div>
<h2 id="another-logistic-regression-example">Another logistic regression example</h2>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dt2 &lt;- mtcars
head(dt2, 3)</code></pre>
<pre><code>
               mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1</code></pre>
</div>
<p>Let’s try to predict <code>vs</code> (binary variable) from <code>mpg</code> (continuous). First, mean-center the predictor variable.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dt2$mpgC &lt;- dt2$mpg - mean(dt2$mpg)</code></pre>
</div>
<p>Let’s create another function to make guesses!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mle2 &lt;- function(b0 = 0.5, b1 = 1.0) {
  predicted_y &lt;- b0 + b1 * dt2$mpgC # dt2$mpgC
  predicted_probs &lt;- logistic(predicted_y)
  sum(dbinom(x = dt2$vs, size = 1, prob = predicted_probs, log = TRUE))
}</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
guesses &lt;- expand.grid(b0_guess = seq(-1, 1, 0.02), # b0 values from -1 to 1, steps of 0.02
                       b1_guess = seq(-1, 1, 0.02)) # b1 values from -1 to 1, steps of 0.02
setDT(guesses) # convert to data.table
guesses[, guessid := 1:.N] # add an id to each row
guesses</code></pre>
<pre><code>
       b0_guess b1_guess guessid
    1:    -1.00       -1       1
    2:    -0.98       -1       2
    3:    -0.96       -1       3
    4:    -0.94       -1       4
    5:    -0.92       -1       5
   ---                          
10197:     0.92        1   10197
10198:     0.94        1   10198
10199:     0.96        1   10199
10200:     0.98        1   10200
10201:     1.00        1   10201</code></pre>
<pre class="r"><code>
guesses[, loglikelihood := mle2(b0 = b0_guess, b1 = b1_guess), by = guessid]
rowid &lt;- which.max(guesses$loglikelihood) # row that has the largest summed loglikelihood
rowid</code></pre>
<pre><code>
[1] 7314</code></pre>
<pre class="r"><code>
guesses[rowid, .(b0_guess, b1_guess)] # coefficients that had the largest loglikelihood</code></pre>
<pre><code>
   b0_guess b1_guess
1:    -0.18     0.44</code></pre>
</div>
<p>Let’s fit the model with <code>glm()</code> now.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
glm2 &lt;- glm(vs ~ mpgC, data = dt2, family = binomial())
coef(glm2)</code></pre>
<pre><code>
(Intercept)        mpgC 
 -0.1857959   0.4304135 </code></pre>
<pre class="r"><code>
guesses[rowid, .(b0_guess, b1_guess)] # coefficients that had the largest loglikelihood</code></pre>
<pre><code>
   b0_guess b1_guess
1:    -0.18     0.44</code></pre>
</div>
<p>The coefficients obtained from <code>glm()</code> and our manual maximum likelihood estimation are very similar!</p>
<p>Note that our maximum likelihood estimation estimation procedure is very inefficient and won’t work when we have more predictors. But the examples here are simple to provide an intuitive explanation of how logistic regression works. In reality, the computer computes derivatives when performing maximum likelihood estimation to find coefficients efficiently.</p>
<h2 id="step-by-step-algebra-convert-between-inverse-logit-and-logit-equations">Step-by-step algebra: convert between inverse logit and logit equations</h2>
<h3 id="inverse-logit-to-logit">Inverse logit to logit</h3>
<p>Let’s start with the inverse logit equation and try to derive the logit equation <span class="math inline">\(log(\frac{p}{1-p})\)</span>. Inverse logit equation:</p>
<p><span class="math display">\[p=\frac{1}{1+e^{-y}}\]</span></p>
<p>Multiply both sides by the denominator of the right-hand side, <span class="math inline">\((1 + e^{-y})\)</span>:</p>
<p><span class="math display">\[p(1 + e^{-y}) = 1\]</span></p>
<p>Expand the left hand side:</p>
<p><span class="math display">\[p + pe^{-y} = 1\]</span></p>
<p>Isolate the term with the exponential (<span class="math inline">\(pe^{-y}\)</span>):</p>
<p><span class="math display">\[pe^{-y} = 1 - p\]</span></p>
<p>Express the left-hand side as a fraction:</p>
<p><span class="math display">\[\frac{p}{e^{y}} = 1 - p\]</span></p>
<p>Multiply both sides by the denominator of the left-hand side, <span class="math inline">\(e^{y}\)</span>:</p>
<p><span class="math display">\[p = e^{y}(1 - p)\]</span></p>
<p>Isolate the term with the exponential, <span class="math inline">\(e^{y}\)</span>:</p>
<p><span class="math display">\[\frac{p}{1-p} = e^{y}\]</span></p>
<p>Eliminate the exponent by apply the natural log to both sides:</p>
<p><span class="math display">\[log(\frac{p}{1-p}) = log(e^{y})\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[log(\frac{p}{1-p}) = y\]</span></p>
<p>Try it for yourself if you don’t understand the final step.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
(x &lt;- exp(2)) # 7.389056</code></pre>
<pre><code>
[1] 7.389056</code></pre>
<pre class="r"><code>
log(x) # 2! </code></pre>
<pre><code>
[1] 2</code></pre>
<pre class="r"><code>
log(exp(2)) # 2! </code></pre>
<pre><code>
[1] 2</code></pre>
<pre class="r"><code>
exp(log(11.3))</code></pre>
<pre><code>
[1] 11.3</code></pre>
</div>
<p>You’ve derived the logit function <span class="math inline">\(log(\frac{p}{1-p})\)</span> from the inverse logit function <span class="math inline">\(p=\frac{1}{1+e^{-y}}\)</span>!</p>
<h3 id="logit-to-inverse-logit">Logit to inverse logit</h3>
<p>Now let’s try the opposite: Derive the inverse logit <span class="math inline">\(p=\frac{1}{1+e^{-y}}\)</span> from the logit function:</p>
<p><span class="math display">\[y=log(\frac{p}{1-p})\]</span></p>
<p>Eliminate the natural log by exponentiating both sides:</p>
<p><span class="math display">\[e^{y}=e^{log(\frac{p}{1-p})}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[e^{y}=\frac{p}{1-p}\]</span></p>
<p>Get rid of the denominator on the right-hand side by multiplying both sides by <span class="math inline">\((1-p)\)</span>:</p>
<p><span class="math display">\[e^{y}(1-p)=p\]</span></p>
<p>Expand the left-hand side:</p>
<p><span class="math display">\[e^{y}-e^{y}p=p\]</span></p>
<p>Since our goal is to have the <span class="math inline">\(p\)</span> on the left-hand side (<span class="math inline">\(p=\frac{1}{1+e^{-y}}\)</span>), we gather the terms with <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[e^{y} = p + e^{y}p\]</span></p>
<p>Isolate the <span class="math inline">\(p\)</span> term:</p>
<p><span class="math display">\[e^{y} = p(1 + e^{y})\]</span></p>
<p><span class="math display">\[\frac{e^{y}}{(1 + e^{y})} = p\]</span></p>
<p>That’s it! This equation might not look like <span class="math inline">\(p=\frac{1}{1+e^{-y}}\)</span>, but if you remember from above, they are the same thing!</p>
<p><span class="math display">\[
\frac{e^{y}}{e^{y}+1}=
(\frac{1}{1+e^{-y}}) * (\frac{e^{y}}{e^{y}})
\]</span></p>
<h2 id="further-resources">Further resources</h2>
<p>I also used these resources to learn how logistic regression works and to create this tutorial.</p>
<ul>
<li><a href="https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a" class="uri">https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a</a></li>
<li><a href="https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389" class="uri">https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389</a></li>
<li><a href="https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102" class="uri">https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102</a></li>
<li><a href="https://christophm.github.io/interpretable-ml-book/logistic.html" class="uri">https://christophm.github.io/interpretable-ml-book/logistic.html</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python" class="uri">https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python</a></li>
<li><a href="https://czep.net/stat/mlelr.pdf">Maximum Likelihood Estimation of Logistic Regression Models: Theory and Implementation</a></li>
<li><a href="https://rpubs.com/aelhabr/logistic-regression-tutorial" class="uri">https://rpubs.com/aelhabr/logistic-regression-tutorial</a></li>
<li><a href="https://daviddalpiaz.github.io/appliedstats/logistic-regression.html" class="uri">https://daviddalpiaz.github.io/appliedstats/logistic-regression.html</a></li>
<li><a href="https://daviddalpiaz.github.io/r4sl/logistic-regression.html" class="uri">https://daviddalpiaz.github.io/r4sl/logistic-regression.html</a></li>
<li><a href="https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/" class="uri">https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/</a></li>
<li><a href="https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/" class="uri">https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/</a></li>
</ul>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/hauselin/rtutorialsite/blob/master/_posts/2019-04-13-logistic-regression/logistic-regression.Rmd/issues/new">create an issue</a> on the source repository.</p>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
